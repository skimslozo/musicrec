{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDl0GId2PrL9"
      },
      "source": [
        "**Import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "4MUFO7VDPdgo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import math\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import seaborn as sns\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Dhc5Oc3I9VCC"
      },
      "outputs": [],
      "source": [
        "class Connect2Model(nn.Module):\n",
        "\n",
        "    def __init__(self, board_size, action_size, device):\n",
        "\n",
        "        super(Connect2Model, self).__init__()\n",
        "\n",
        "        self.device = device\n",
        "        self.size = board_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=self.size, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
        "\n",
        "        # Two heads on our network\n",
        "        self.action_head = nn.Linear(in_features=64, out_features=self.action_size)\n",
        "        self.value_head = nn.Linear(in_features=64, out_features=1)\n",
        "\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        action_logits = self.action_head(x)\n",
        "        value_logit = self.value_head(x)\n",
        "\n",
        "        return F.softmax(action_logits, dim=1), value_logit\n",
        "\n",
        "    def predict(self, board):\n",
        "        board = torch.FloatTensor(board.astype(np.float32)).to(self.device)\n",
        "        board = board.view(1, self.size)\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            pi, v = self.forward(board)\n",
        "\n",
        "        return pi.data.cpu().numpy()[0], v.data.cpu().numpy()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "fme3ZDd59oPb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "class Trainer:\n",
        "\n",
        "    def __init__(self, dl_model , model, args):\n",
        "        self.dl_model = dl_model\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "        self.mcts = MCTS_DL(self.dl_model, self.model, self.args)\n",
        "        self.scores_mcts_dl = np.array([])\n",
        "        self.scores_greedy = np.array([])\n",
        "        self.scores_random = np.array([])\n",
        "        self.scores_mcts = np.array([])\n",
        "\n",
        "    def exceute_episode(self, tmp, plotting, random, greedy):\n",
        "\n",
        "        train_examples = []\n",
        "        state = []\n",
        "\n",
        "        while True:\n",
        "\n",
        "            self.mcts = MCTS_DL(self.dl_model, self.model, self.args)\n",
        "            root = self.mcts.run(state)\n",
        "\n",
        "            action_probs = [0 for _ in range(self.model.n_songs)]\n",
        "            for k, v in root.children.items():\n",
        "                action_probs[k] = v.visit_count\n",
        "\n",
        "            action_probs = action_probs / np.sum(action_probs)\n",
        "            train_examples.append((state, action_probs))\n",
        "\n",
        "            action = root.select_action(temperature=tmp)\n",
        "            state = self.model.get_next_state(state, action)\n",
        "            reward = self.model.get_reward(state)\n",
        "\n",
        "            \n",
        "            if reward is not None:\n",
        "                ret = []\n",
        "                for hist_state, hist_action_probs in train_examples:\n",
        "                    # [Board, actionProbabilities, Reward]\n",
        "                    ret.append((self.model.state_to_input(hist_state), hist_action_probs, reward))\n",
        "                print(hist_state, reward)\n",
        "\n",
        "                if plotting:\n",
        "\n",
        "                    self.scores_mcts_dl = np.append(self.scores_mcts_dl, reward)\n",
        "                    self.scores_greedy = np.append(self.scores_greedy, get_greedy_trajectory(self.model))\n",
        "                    self.scores_random = np.append(self.scores_random, get_random_trajectory(self.model))\n",
        "                    self.scores_mcts = np.append(self.scores_mcts, get_MCTS_trajectory(self.model))\n",
        "                    plt.plot(self.scores_mcts_dl, label='DeepMCTS')\n",
        "                    plt.plot(self.scores_mcts, label='Vanilla MCTS')\n",
        "                    plt.plot(self.scores_greedy, label='Greedy policy')\n",
        "                    plt.plot(self.scores_random, label='Random policy')\n",
        "                    plt.legend()\n",
        "                    plt.show()\n",
        "                    \n",
        "                return ret\n",
        "\n",
        "    def learn(self, temperature, random, greedy):\n",
        "        for i in range(1, self.args['numIters'] + 1):\n",
        "\n",
        "            print(\"{}/{}\".format(i, self.args['numIters']))\n",
        "\n",
        "            train_examples = []\n",
        "\n",
        "            for eps in range(self.args['numEps']):\n",
        "                iteration_train_examples = self.exceute_episode(temperature, False, random, greedy)\n",
        "                train_examples.extend(iteration_train_examples)\n",
        "\n",
        "            self.exceute_episode(0, True, random, greedy)\n",
        "            shuffle(train_examples)\n",
        "            self.train(train_examples)\n",
        "            filename = self.args['checkpoint_path']\n",
        "            self.save_checkpoint(folder=\".\", filename=filename)\n",
        "\n",
        "    def train(self, examples):\n",
        "        optimizer = optim.Adam(self.dl_model.parameters(), lr=5e-4)\n",
        "        pi_losses = []\n",
        "        v_losses = []\n",
        "\n",
        "        for epoch in range(self.args['epochs']):\n",
        "          self.dl_model.train()\n",
        "\n",
        "          batch_idx = 0\n",
        "\n",
        "          while batch_idx < int(len(examples) / self.args['batch_size']):\n",
        "            sample_ids = np.random.randint(len(examples), size=self.args['batch_size'])\n",
        "            boards, pis, vs = list(zip(*[examples[i] for i in sample_ids]))\n",
        "            boards = torch.FloatTensor(np.array(boards).astype(np.float64))\n",
        "            target_pis = torch.FloatTensor(np.array(pis))\n",
        "            target_vs = torch.FloatTensor(np.array(vs).astype(np.float64))\n",
        "\n",
        "            # predict\n",
        "            boards = boards.contiguous().cuda()\n",
        "            target_pis = target_pis.contiguous().cuda()\n",
        "            target_vs = target_vs.contiguous().cuda()\n",
        "\n",
        "            # compute output\n",
        "            out_pi, out_v = self.dl_model(boards)\n",
        "            l_pi = self.loss_pi(target_pis, out_pi)\n",
        "            l_v = self.loss_v(target_vs, out_v)\n",
        "            total_loss = l_pi + l_v\n",
        "\n",
        "            pi_losses.append(float(l_pi))\n",
        "            v_losses.append(float(l_v))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            batch_idx += 1\n",
        "    \n",
        "\n",
        "          print()\n",
        "          print(\"Policy Loss\", np.mean(pi_losses))\n",
        "          print(\"Value Loss\", np.mean(v_losses))\n",
        "          # print(\"Examples:\")\n",
        "          # print(out_pi[0].detach())\n",
        "          # print(target_pis[0])\n",
        "          \n",
        "\n",
        "    def loss_pi(self, targets, outputs):\n",
        "        loss = -(targets * torch.log(outputs)).sum(dim=1)\n",
        "        return loss.mean()\n",
        "\n",
        "    def loss_v(self, targets, outputs):\n",
        "        loss = torch.sum((targets-outputs.view(-1))**2)/targets.size()[0]\n",
        "        return loss\n",
        "\n",
        "    def save_checkpoint(self, folder, filename):\n",
        "        if not os.path.exists(folder):\n",
        "            os.mkdir(folder)\n",
        "\n",
        "        filepath = os.path.join(folder, filename)\n",
        "        torch.save({\n",
        "            'state_dict': self.dl_model.state_dict(),\n",
        "        }, filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "PLQCEz-NFk6o"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import itertools\n",
        "import scipy.spatial.distance as distlib\n",
        "import random as rand\n",
        "\n",
        "class RecommendationSystem:\n",
        "    def __init__(self, dataset_with_bins, n_features, n_bins):\n",
        "        '''\n",
        "        We assume here that the columns of the dataset here are already the binary percentile bins for all features \n",
        "        For sanity check: n_features x n_bins = length(dataset_with_bins)\n",
        "        '''\n",
        "        self.data = dataset_with_bins # assuming this is the whole dataset split that we want to work with\n",
        "        self.data.index = np.arange(np.shape(self.data)[0])\n",
        "        self.n_features = n_features\n",
        "        self.n_bins = n_bins\n",
        "        self.k_t = 10 # for now, just queue the user 10 songs to choose from\n",
        "        self.k_s = 10\n",
        "        self.var_param = 100 # parameter that determines the \"variance\" of the user reference distribution that the user song picks are generated from\n",
        "        self.initial_songs = self.gen_user_song_picks() # assuming this is still a pandas df of k_s rows, but only containing the song rows that the user prefers\n",
        "        \n",
        "        # Initialize preferences\n",
        "        self.init_song_preferences()\n",
        "        self.init_transition_preferences()\n",
        "\n",
        "    def init_song_preferences(self):\n",
        "        # Initialize preference array\n",
        "        self.phi_s = (1/((self.k_s + 1) * self.n_bins)) * np.ones((self.n_features * self.n_bins, 1))\n",
        "        tmp = (np.sum(self.initial_songs.values, axis = 0)) * (1/(self.k_s + 1))\n",
        "        self.phi_s = self.phi_s + np.reshape(tmp, (len(tmp), 1))\n",
        "\n",
        "    def theta_t(self, idx_a, idx_b):\n",
        "        '''\n",
        "        Input: indices of songs a and b within the provided dataset (int)\n",
        "\n",
        "        Output: vector theta_t, assuming the feature sequence of 1-i, 1-2, ..., 1-n_bins, 2-1, ..., n_bins-1, n_bins-2, ..., n_bins-n_bins\n",
        "        '''\n",
        "        indices = np.array([], dtype=int)\n",
        "        for i in range(self.n_features):\n",
        "            a_bin_idx = np.where(self.data.loc[idx_a][self.data.columns[i*self.n_bins:(i+1)*self.n_bins]] == 1.0)[0]\n",
        "            b_bin_idx = np.where(self.data.loc[idx_b][self.data.columns[i*self.n_bins:(i+1)*self.n_bins]] == 1.0)[0]\n",
        "            indices = np.append(indices, int(i*(self.n_bins*self.n_bins) + a_bin_idx*self.n_bins + b_bin_idx))\n",
        "        out = np.zeros((self.n_bins * self.n_bins * self.n_features, 1))\n",
        "        out[indices] = 1\n",
        "        return out\n",
        "\n",
        "    def init_transition_preferences(self):\n",
        "\n",
        "        # Take the upper-median preference split\n",
        "        self.Rs = np.sum(np.matmul(self.data.values, self.phi_s), axis=1)\n",
        "        self.Mstar = self.data.copy()\n",
        "        self.Mstar['Rs'] = self.Rs\n",
        "        self.Mstar.sort_values('Rs', inplace = True, ascending = False)\n",
        "        self.Mstar = self.Mstar[:np.shape(self.Mstar)[0] // 2]\n",
        "        self.Mstar['old_index'] = self.Mstar.index\n",
        "        self.Mstar.index = np.arange(np.shape(self.Mstar)[0])\n",
        "\n",
        "        # Generate 10th percentile distance of all pairwise distances from M (not M*)\n",
        "        self.diff = distlib.pdist(self.data.values, 'cosine')\n",
        "        self.delta = np.percentile(self.diff, 10, axis=0)\n",
        "        self.distances = distlib.squareform(self.diff)\n",
        "        np.fill_diagonal(self.distances, np.inf)\n",
        "\n",
        "        # Generate a representative subset of M*\n",
        "        self.representatives, self.clusters_t = self.delta_medoids(self.Mstar, self.delta, True)\n",
        "        song_prev = np.random.choice(self.representatives)\n",
        "\n",
        "        if self.k_t > len(self.representatives):\n",
        "            print(\"[WARNING] k_t parameter too large for generated representative subset. Consider using a larger dataset or reducing k_t value\")\n",
        "            print(\"Setting k_t =\", len(self.representatives), \"to match the generated number of representatives through delta-medoids\")\n",
        "            self.k_t = len(self.representatives)\n",
        "\n",
        "        song_prev = np.random.choice(self.representatives)\n",
        "        self.representatives = np.delete(self.representatives, np.argwhere(self.representatives == song_prev))\n",
        "\n",
        "        self.phi_t = (1/(self.k_t * self.n_bins*self.n_bins)) * np.ones((self.n_features * self.n_bins * self.n_bins, 1))\n",
        "\n",
        "        for i in range(self.k_t-1):\n",
        "            next_song, theta = self.pick_next_song(song_prev, self.user_ref_samples_t[:, i])\n",
        "            self.phi_t += (1/(self.k_t)) * theta\n",
        "            song_prev = next_song\n",
        " \n",
        "\n",
        "    def one_shot_delta(self, data, delta, clusters, init):\n",
        "        # Remember to change the distance metric in case we change delta distance definition\n",
        "        if init:\n",
        "            distances = distlib.pdist(data[data.columns[:-2]], 'cosine')\n",
        "        else:\n",
        "            distances = distlib.pdist(data, 'cosine') # Same as for delta medoids method\n",
        "        distances = distlib.squareform(distances)\n",
        "        representatives_init = np.array(list(clusters.keys()), dtype = 'int32')\n",
        "        clusters = dict(zip(representatives_init, [np.array([], dtype='int32') for _ in range(len(representatives_init))])) # We want to start with a dictionary with the keys being there, but the elements being empty, as all elements will be reassigned anyways\n",
        "        for index, row in data.iterrows():\n",
        "            dist = 1e6\n",
        "            representatives = np.array(list(clusters.keys()), dtype = 'int32')\n",
        "            if len(representatives) > 0:\n",
        "                rep = np.intersect1d(np.where(distances[:, index] == np.min(distances[:, index][representatives]))[0], representatives) # Take the intersection to make sure we select the correct index belonging to the set of representatives\n",
        "                \n",
        "                if len(rep) > 1:\n",
        "                    if index not in rep: # Check if already in rep --> we do not want to add it to another cluster if it is already a representative (in case index and another song have the same feature vectors, miraculously)\n",
        "                        rep = rep[0]\n",
        "                    else:\n",
        "                        rep = index\n",
        "                else:\n",
        "                    rep = int(rep)\n",
        "                dist = distances[rep, index]\n",
        "            if dist <= delta:\n",
        "                clusters[rep] = np.append(clusters[rep], index)\n",
        "            else: \n",
        "                clusters[index] = np.array([index])\n",
        "\n",
        "        dbg = np.sum([len(val) for val in clusters.values()])\n",
        "        if dbg != np.shape(data)[0]:\n",
        "            print(\"WARNING: stuff going wrong here, generated list of cluster elements does not sum up to the total size of the song dataset selected\")\n",
        "        return clusters\n",
        "\n",
        "    def delta_medoids(self, data, delta, init):\n",
        "        if init:\n",
        "            distances = distlib.pdist(data[data.columns[:-2]], 'cosine')\n",
        "        else:\n",
        "            distances = distlib.pdist(data, 'cosine') # When we want to use it to generate normal clusters of the whole dataset - we do not have anything appended, so use all columns\n",
        "        distances = distlib.squareform(distances)\n",
        "        np.fill_diagonal(distances, 0)\n",
        "        exit_loop = False\n",
        "        i = 0\n",
        "        clusters = {}\n",
        "        while not exit_loop:\n",
        "            i +=1\n",
        "            clusters = self.one_shot_delta(data, delta, clusters, init)\n",
        "            if 1 != i:\n",
        "                representatives_prev = representatives\n",
        "            else:\n",
        "                representatives_prev = np.array([], dtype = 'int32')\n",
        "            representatives = np.array([], dtype = 'int32')\n",
        "            for cluster in clusters.items():\n",
        "                cluster = cluster[1]\n",
        "                cluster_dists = distlib.pdist(data.loc[cluster], 'cosine')\n",
        "                cluster_dists = distlib.squareform(cluster_dists)\n",
        "                argmin = np.argmin(np.sum(cluster_dists, axis=0))\n",
        "                representatives = np.append(representatives, cluster[argmin])\n",
        "            \n",
        "            if np.array_equal(np.sort(representatives), np.sort(representatives_prev)):\n",
        "                exit_loop = True\n",
        "\n",
        "        # Convert back to indices of the original data array\n",
        "        if init:\n",
        "            representatives = data['old_index'][np.in1d(data.index, representatives)].values # Do not need to convert back if clustering songs together for action space reduction, as we preserve the old indices anyways\n",
        "        return representatives, clusters\n",
        "\n",
        "    def pick_next_song(self, previous_song, reference):\n",
        "        reference = np.reshape(reference, (len(reference), 1))\n",
        "        min_dist = 1e06\n",
        "        for song in self.representatives:\n",
        "            theta_current = self.theta_t(previous_song, song)\n",
        "            dist = distlib.cosine(theta_current, reference)\n",
        "            if dist < min_dist:\n",
        "                min_theta = theta_current\n",
        "                min_song = song\n",
        "                min_dist = dist\n",
        "\n",
        "        self.representatives = np.delete(self.representatives, np.argwhere(self.representatives == min_song)[0])\n",
        "        return min_song, min_theta\n",
        "    \n",
        "    def gen_user_song_picks(self):\n",
        "        \"\"\"\n",
        "        Returns a subset of the dataframe of k_s songs closest to the random samples generated from the user preference distribution, simulating picked user reference\n",
        "        The user reference distribution is generated here and can be accessed through self.user_ref\n",
        "        \"\"\"\n",
        "        self.user_ref_s, self.user_ref_samples_s, self.user_ref_t, self.user_ref_samples_t  = self.gen_user_ref()\n",
        "        sample_index = np.arange(np.shape(self.data)[0], np.shape(self.data)[0] + self.k_s, dtype='int32')\n",
        "        to_append = pd.DataFrame(np.transpose(self.user_ref_samples_s), index = sample_index)\n",
        "        sample = self.data.copy().append(to_append) # Doing this so we can make use of the pdist lib\n",
        "        dists_with_samples = distlib.pdist(sample, metric='cosine')\n",
        "        dists_with_samples = distlib.squareform(dists_with_samples)\n",
        "        np.fill_diagonal(dists_with_samples, np.inf)\n",
        "        user_selected_songs = np.array([], dtype='int32')\n",
        "\n",
        "        for i in sample_index:\n",
        "            min_ind = np.argmin(dists_with_samples[:sample_index[0], i])\n",
        "            user_selected_songs = np.append(user_selected_songs, min_ind)\n",
        "            dists_with_samples[min_ind, :] = np.inf #Do not allow to pick this row anymore\n",
        "        return self.data.loc[user_selected_songs]\n",
        "        \n",
        "\n",
        "    def gen_user_ref(self):\n",
        "        '''\n",
        "        Generates a random \"ground-truth\" user song and transitions preference distributions over features (kind of\n",
        "        like a ground-truth phi_s and phi_t), as well as k_s for songs (and k_t, for transitions, respectively) samples from it, that are later used to \n",
        "        simulate user song preferences and transition preferences that the user preference vectors phi_s and phi_t are learned from\n",
        "\n",
        "        Input: var_param: a parameter that alters the variance of the distribution over each feature of the ground truth user preference vectors, int\n",
        "\n",
        "        Output: - user_reference_s (numpy array, n_bins x n_features, 1): ground-truth usser song preference distribution over each feature\n",
        "                - user_samples_s (numpy array, n_bins x n_features, k_s): k_s samples of the \"ground truth phi_s\", used to select initial, user-preferred songs\n",
        "                - user_reference_t (numpy array, n_bins*n_bins x n_features, 1): ground-truth user transition preference distribution over each feature\n",
        "                - user_samples_t (numpy array, n_bins*n_bins x n_features, k_t): k_t samples of the \"ground truth phi_t\", used to select songs that the user preference phi_t will be estimated from \n",
        "        '''\n",
        "    \n",
        "        user_reference_s = np.array([])\n",
        "        user_reference_t = np.array([])\n",
        "\n",
        "        user_samples_s = np.zeros((self.n_features * self.n_bins, self.k_s))\n",
        "        user_samples_t = np.zeros((self.n_features * self.n_bins * self.n_bins, self.k_t))\n",
        "\n",
        "        for i in range(self.n_features):\n",
        "            vec_s = np.random.randint(1, self.var_param, (self.n_bins, ))\n",
        "            vec_s = vec_s/np.sum(vec_s)\n",
        "            user_reference_s = np.append(user_reference_s, vec_s)\n",
        "            sample_bins_s = np.array(rand.choices(np.arange(self.n_bins, dtype='int32'), vec_s, k = self.k_s))\n",
        "            user_samples_s[i*self.n_bins + sample_bins_s, np.arange(self.k_s, dtype='int32')] = 1\n",
        "            \n",
        "            vec_t = np.random.randint(1, self.var_param, (self.n_bins * self.n_bins, ))\n",
        "            vec_t = vec_t/np.sum(vec_t)\n",
        "            user_reference_t = np.append(user_reference_t, vec_t)\n",
        "            sample_bins_t = np.array(rand.choices(np.arange(self.n_bins * self.n_bins, dtype='int32'), vec_t, k = self.k_t))\n",
        "            user_samples_t[i*self.n_bins*self.n_bins + sample_bins_t, np.arange(self.k_t, dtype='int32')] = 1\n",
        "\n",
        "        return user_reference_s, user_samples_s, user_reference_t, user_samples_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "SND1-OJ3D4hI"
      },
      "outputs": [],
      "source": [
        "def ucb_score(parent, child):\n",
        "    \"\"\"\n",
        "    The score for an action that would transition between the parent and child.\n",
        "    \"\"\"\n",
        "    if child.prior < 0.0001:\n",
        "      return -100000000\n",
        "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "    if child.visit_count > 0:\n",
        "        # The value of the child is from the perspective of the opposing player\n",
        "        value_score = child.value()\n",
        "    else:\n",
        "        value_score = 100000\n",
        "\n",
        "    return value_score + prior_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0DKpyChhthYt"
      },
      "outputs": [],
      "source": [
        "class MDP:\n",
        "    def __init__(self, file_name, playlist_length, action_space_size):\n",
        "\n",
        "        df = pd.read_csv(file_name, index_col=None)\n",
        "\n",
        "        self.features = ['duration', 'key_confidence', 'end_of_fade_in', 'mode_confidence', 'start_of_fade_out', 'tempo',\n",
        "                    'artist_hotttnesss', 'song_hotttnesss']\n",
        "\n",
        "        for feature in self.features:\n",
        "          filter = df[feature] > 0\n",
        "          df = df[filter]\n",
        "        \n",
        "        self.filtered_df = df[self.features]\n",
        "        self.filtered_df.dropna(inplace=True)\n",
        "        self.filtered_df = self.filtered_df.head(action_space_size)\n",
        "        self.filtered_df = self.filtered_df.reset_index(drop=True)\n",
        "        self.whole_df = self.filtered_df.copy()\n",
        "        self.dataSize = np.shape(self.whole_df)[0]\n",
        "\n",
        "        self.filtered_df = self.filtered_df.head(20) # Reduce the dataset (hence the action space) here \n",
        "\n",
        "        self.n_rows = len(self.filtered_df.index)\n",
        "\n",
        "        self.n_songs = self.n_rows\n",
        "        self.n_bins = 10\n",
        "        self.n_features = len(self.features)\n",
        "        self.id_song_to_vec = dict()\n",
        "        self.songs = set()        \n",
        "        self.song_id_to_song_row = []\n",
        "        self.song_priors = []\n",
        "        self.playlist_length = playlist_length\n",
        "        self.id_song_to_features = dict()\n",
        "\n",
        "        song_id = 0\n",
        "      \n",
        "        for song, row in self.filtered_df.iterrows():\n",
        "            self.id_song_to_vec[song_id] = np.zeros(80)\n",
        "            self.id_song_to_features[song_id] = np.zeros(self.n_features)\n",
        "\n",
        "            for f in range(self.n_features):\n",
        "              feature = self.features[f]\n",
        "              self.id_song_to_features[song_id][f] = row[feature]\n",
        "\n",
        "            self.songs.add(song_id)\n",
        "            self.song_id_to_song_row.append(row)\n",
        "            self.song_priors.append(1)\n",
        "            song_id += 1\n",
        "\n",
        "        self.songs_list = [i for i in range(self.n_songs)]\n",
        "\n",
        "        for f in range(self.n_features):\n",
        "\n",
        "            feature = self.features[f]\n",
        "            sorted_df = self.filtered_df.sort_values(by=[feature])\n",
        "            i = 0\n",
        "            for idx, row in sorted_df.iterrows():\n",
        "                bin = i * self.n_bins // self.n_rows\n",
        "                self.id_song_to_vec[idx][f * 10 + bin] = 1\n",
        "                i += 1\n",
        "\n",
        "        # Repeating the procedure above for the full dataset\n",
        "        self.rs_input_dict = dict(zip(np.arange(self.dataSize), [np.zeros(80) for _ in range(self.dataSize)]))\n",
        "        for f in range(self.n_features):\n",
        "          feature = self.features[f]\n",
        "          sorted_df = self.whole_df.sort_values(by=[feature])\n",
        "          i = 0\n",
        "          for idx, row in sorted_df.iterrows():\n",
        "              bin = i * self.n_bins // self.dataSize\n",
        "              self.rs_input_dict[idx][f * 10 + bin] = 1\n",
        "              i += 1\n",
        "\n",
        "        df_input = pd.DataFrame.from_dict(self.rs_input_dict, orient='index')\n",
        "\n",
        "        self.rs = RecommendationSystem(df_input, self.n_features, self.n_bins)\n",
        "        self.phi_s = self.rs.phi_s.reshape(80)\n",
        "        self.phi_t = self.rs.phi_t.reshape(800)\n",
        "\n",
        "        self.transition_reward = np.zeros((self.n_songs, self.n_songs))\n",
        "\n",
        "        self.song_reward = np.zeros(self.n_songs)\n",
        "        for i in range(self.n_songs):\n",
        "          self.song_reward[i] = np.dot(self.phi_s, self.theta_s(i))\n",
        "          for j in range(self.n_songs):\n",
        "            self.transition_reward[i,j] = np.dot(self.phi_t, self.get_theta_t(i, j))\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "      return state + [action]\n",
        "      \n",
        "    def get_valid_actions(self, state):\n",
        "        # All actions are invalid by default\n",
        "        valid_actions = [0] * self.n_songs\n",
        "\n",
        "        for song in range(self.n_songs):\n",
        "            if song not in state:\n",
        "                valid_actions[song] = 1\n",
        "\n",
        "        return valid_actions\n",
        "\n",
        "    def state_to_input(self, state):\n",
        "      input = np.ones(self.playlist_length*self.n_features*10)*-1\n",
        "      for i in range(len(state)):\n",
        "        song = state[i]\n",
        "        input[i*self.n_features*10:(i+1)*self.n_features*10] = self.id_song_to_vec[song]\n",
        "      return input\n",
        "\n",
        "    def get_reward(self, final_state):\n",
        "      if len(final_state) < self.playlist_length:\n",
        "        return None\n",
        "      elif len(final_state) == self.playlist_length:\n",
        "        state = []\n",
        "        trajectory_states = [state]\n",
        "        trajectory_actions = []\n",
        "        \n",
        "        for song in final_state:\n",
        "          state = self.get_next_state(state, song)\n",
        "          trajectory_states.append(state)\n",
        "          trajectory_actions.append(song)\n",
        "          \n",
        "        return self.payoff_trajectory(trajectory_states, trajectory_actions)\n",
        "      else:\n",
        "        print(\"Error: length > self.playlist_length\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    def get_theta_t(self, s1, s2):\n",
        "        theta_t = np.zeros(800)\n",
        "        for f in range(len(self.features)):\n",
        "            for i in range(10):\n",
        "                for j in range(10):\n",
        "                    if self.id_song_to_vec[s1][f*10 + i] == 1 and  self.id_song_to_vec[s2][f*10 + j]:\n",
        "                        theta_t[f*100 + i*10 + j] = 1\n",
        "        return  theta_t\n",
        "\n",
        "    def theta_s(self, s):\n",
        "        return self.id_song_to_vec[s]\n",
        "\n",
        "    def R(self, s, a):\n",
        "        Rs = self.song_reward[a]\n",
        "        Rt = 0\n",
        "        for i in range(len(s)):\n",
        "            song_past = s[len(s)-i-1]\n",
        "            Rt += 1/((i+1)**(0.1)) * self.transition_reward[song_past,a]\n",
        "        return Rs + Rt\n",
        "\n",
        "\n",
        "    def payoff_trajectory(self, trajectory_states, trajectory_actions):\n",
        "      payoff = 0\n",
        "\n",
        "      for t in range(len(trajectory_actions)):\n",
        "        payoff +=  self.R(trajectory_states[t], trajectory_actions[t])\n",
        "      return payoff\n",
        "\n",
        "    def MC_value(self, s):\n",
        "      count = 0\n",
        "      sum_values = 0\n",
        "      state = s\n",
        "      # Past episodes\n",
        "      episode_states = [[]]\n",
        "      episode_actions = []\n",
        "\n",
        "      for i in range(model.playlist_length):\n",
        "        if i < len(s):\n",
        "          state = s[:i+1]\n",
        "          action = s[i]\n",
        "        else:\n",
        "          action_probs = list(model.song_priors)\n",
        "          for song_id in range(model.n_songs):\n",
        "            if song_id in state:\n",
        "              action_probs[song_id] = 0\n",
        "\n",
        "          action_probs = np.array(action_probs)/np.sum(np.array(action_probs))\n",
        "          action = np.random.choice(model.songs_list, 1, p=action_probs)[0]\n",
        "          state = episode_states[-1] + [action]\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "\n",
        "      # print(\"MC estimate:\", s, episode_states, episode_actions)\n",
        "          \n",
        "      # Set MC \n",
        "      payoff = self.payoff_trajectory(episode_states, episode_actions)\n",
        "      \n",
        "      return payoff\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, prior):\n",
        "        self.visit_count = 0\n",
        "        self.prior = prior\n",
        "        self.value_sum = 0\n",
        "        self.children = {}\n",
        "        self.state = None\n",
        "\n",
        "    def expanded(self):\n",
        "        return len(self.children) > 0\n",
        "\n",
        "    def value(self):\n",
        "        if self.visit_count == 0:\n",
        "            return 0\n",
        "        return self.value_sum / self.visit_count\n",
        "\n",
        "    def select_action(self, temperature):\n",
        "        \"\"\"\n",
        "        Select action according to the visit count distribution and the temperature.\n",
        "        \"\"\"\n",
        "        visit_counts = np.array([child.visit_count for child in self.children.values()])\n",
        "        actions = [action for action in self.children.keys()]\n",
        "        if temperature == 0:\n",
        "            action = actions[np.argmax(visit_counts)]\n",
        "        elif temperature == float(\"inf\"):\n",
        "            action = np.random.choice(actions)\n",
        "        else:\n",
        "            # See paper appendix Data Generation\n",
        "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
        "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
        "            action = np.random.choice(actions, p=visit_count_distribution)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def select_child(self):\n",
        "        \"\"\"\n",
        "        Select the child with the highest UCB score.\n",
        "        \"\"\"\n",
        "        best_score = -np.inf\n",
        "        best_action = -1\n",
        "        best_child = None\n",
        "\n",
        "        for action, child in self.children.items():\n",
        "            score = ucb_score(self, child)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_action = action\n",
        "                best_child = child\n",
        "\n",
        "        return best_action, best_child\n",
        "\n",
        "    def expand(self, state, action_probs):\n",
        "        \"\"\" \n",
        "        We expand a node and keep track of the prior policy probability given by neural network\n",
        "        \"\"\"\n",
        "        self.state = state\n",
        "        for song_id in range(len(action_probs)):\n",
        "              self.children[song_id] = Node(action_probs[song_id])\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        Debugger pretty print node info\n",
        "        \"\"\"\n",
        "        prior = \"{0:.2f}\".format(self.prior)\n",
        "        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "crbFGO3wAKoZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class MCTS_DL:\n",
        "\n",
        "    def __init__(self, dl_model, model, args):\n",
        "        self.dl_model = dl_model\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "    def run(self, state):\n",
        "        root = Node(0)\n",
        "\n",
        "        # EXPAND root\n",
        "        action_probs, value = self.dl_model.predict(self.model.state_to_input(state))\n",
        "        valid_actions = self.model.get_valid_actions(state)\n",
        "        action_probs = action_probs * valid_actions  # mask invalid moves\n",
        "        action_probs /= np.sum(action_probs)\n",
        "        root.expand(state, action_probs)\n",
        "\n",
        "        for _ in range(self.args['num_simulations']):\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "\n",
        "            # SELECT\n",
        "            while node.expanded():\n",
        "                action, node = node.select_child()\n",
        "                search_path.append(node)\n",
        "\n",
        "            parent = search_path[-2]\n",
        "            state = parent.state\n",
        "            # Now we're at a leaf node and we would like to expand\n",
        "            # Players always play from their own perspective\n",
        "            next_state = self.model.get_next_state(state, action)\n",
        "            # Get the board from the perspective of the other player\n",
        "\n",
        "            # The value of the new state from the perspective of the other player\n",
        "            value = self.model.get_reward(next_state)\n",
        "            if value is None:\n",
        "                # If the game has not ended:\n",
        "                # EXPAND\n",
        "                action_probs, value = self.dl_model.predict(self.model.state_to_input(next_state))\n",
        "                valid_actions = self.model.get_valid_actions(next_state)\n",
        "                action_probs = action_probs * valid_actions  # mask invalid moves\n",
        "                action_probs /= np.sum(action_probs)\n",
        "                node.expand(next_state, action_probs)\n",
        "\n",
        "            self.backpropagate(search_path, value)\n",
        "\n",
        "        return root\n",
        "\n",
        "    def backpropagate(self, search_path, value):\n",
        "        \"\"\"\n",
        "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
        "        to the root.\n",
        "        \"\"\"\n",
        "        for node in reversed(search_path):\n",
        "            node.value_sum += value \n",
        "            node.visit_count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "0v-iYHfDEIwe"
      },
      "outputs": [],
      "source": [
        "class MCTS:\n",
        "\n",
        "    def __init__(self, model, args):\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "    def run(self, root_state):\n",
        "        print(\"\")\n",
        "        root = Node(0)\n",
        "        # EXPAND root\n",
        "        action_probs = list(self.model.song_priors)\n",
        "        for song_id in range(self.model.n_songs):\n",
        "          if song_id in root_state:\n",
        "            action_probs[song_id] = 0\n",
        "        action_probs /= np.sum(action_probs)\n",
        "        root.expand(root_state, action_probs)\n",
        "\n",
        "        for _ in range(self.args['num_simulations']):\n",
        "\n",
        "            node = root\n",
        "            search_path = [node]\n",
        "\n",
        "            # SELECT\n",
        "            while node.expanded():              \n",
        "                action, node = node.select_child()\n",
        "                search_path.append(node)\n",
        "            parent = search_path[-2]\n",
        "            state = parent.state\n",
        "            # # Now we're at a leaf node and we would like to expand\n",
        "            # # Players always play from their own perspective\n",
        "            # next_state, _ = self.game.get_next_state(state, action=action)\n",
        "            # # Get the board from the perspective of the other player\n",
        "            # next_state = self.game.get_canonical_board(next_state, )\n",
        "\n",
        "            # The value of the new state from the perspective of the other player\n",
        "            # value = self.game.get_reward_for_player(next_state)\n",
        "            next_state = parent.state + [action]\n",
        "            \n",
        "            value = model.MC_value(next_state)\n",
        "           \n",
        "            action_probs = list(self.model.song_priors)\n",
        "            for song_id in range(self.model.n_songs):\n",
        "              if song_id in parent.state or song_id == action:\n",
        "                action_probs[song_id] = 0\n",
        "\n",
        "            node.expand(next_state, action_probs)\n",
        "        \n",
        "\n",
        "            # if value is None:\n",
        "                # If the game has not ended:\n",
        "                # EXPAND\n",
        "                # action_probs, value = model.predict(next_state)\n",
        "                # valid_moves = self.game.get_valid_moves(next_state)\n",
        "                # action_probs = action_probs * valid_moves  # mask invalid moves\n",
        "                # action_probs /= np.sum(action_probs)\n",
        "                # node.expand(next_state, action_probs)\n",
        "\n",
        "            self.backpropagate(search_path, value)\n",
        "\n",
        "        return root\n",
        "\n",
        "    def backpropagate(self, search_path, value):\n",
        "        \"\"\"\n",
        "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
        "        to the root.\n",
        "        \"\"\"\n",
        "        for node in reversed(search_path):\n",
        "            node.value_sum += value\n",
        "            node.visit_count += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "zhegWXk1VYXG"
      },
      "outputs": [],
      "source": [
        "def get_random_trajectory(model):\n",
        "  songs = set(model.songs)\n",
        "  state = []\n",
        "  trajectory_states = [state]\n",
        "  trajectory_actions = []\n",
        "  for i in range(playlist_length):\n",
        "    action = random.sample(songs, 1)[0]\n",
        "    state = model.get_next_state(state, action)\n",
        "    trajectory_states.append(state)\n",
        "    trajectory_actions.append(action)\n",
        "    songs.remove(action)\n",
        "  cum_rewards = model.payoff_trajectory(trajectory_states, trajectory_actions)\n",
        "  return cum_rewards\n",
        "\n",
        "def get_greedy_trajectory(model):\n",
        "  songs = set(model.songs)\n",
        "  state = []\n",
        "  trajectory_states = [state]\n",
        "  trajectory_actions = []\n",
        "  for i in range(playlist_length):\n",
        "    max_score = -np.inf\n",
        "    max_action = -1\n",
        "    for try_action in songs:\n",
        "      try_state = model.get_next_state(state, try_action)\n",
        "      try_trajectory_states = list(trajectory_states)\n",
        "      try_trajectory_actions = list(trajectory_actions)\n",
        "      try_trajectory_states.append(try_state)\n",
        "      try_trajectory_actions.append(try_action)\n",
        "      score = model.payoff_trajectory(try_trajectory_states, try_trajectory_actions)\n",
        "      if score > max_score:\n",
        "        max_action = try_action\n",
        "        max_score = score\n",
        "    \n",
        "    action = max_action\n",
        "    state = model.get_next_state(state, action)\n",
        "    trajectory_states.append(state)\n",
        "    trajectory_actions.append(action)\n",
        "    songs.remove(action)\n",
        "  cum_rewards = model.payoff_trajectory(trajectory_states, trajectory_actions)\n",
        "  return cum_rewards\n",
        "\n",
        "def get_MCTS_trajectory(model):\n",
        "  # MCTS\n",
        "  mcts = MCTS(model, args)\n",
        "  trajectory_states = [[]]\n",
        "  trajectory_actions = []\n",
        "  state = []\n",
        "  for i in range(playlist_length):\n",
        "    root = mcts.run(state)\n",
        "    counts = [node.visit_count for node in root.children.values()]\n",
        "    max_val = max(counts)\n",
        "    song = counts.index(max_val)\n",
        "    state = list(root.children.values())[song].state\n",
        "    trajectory_states.append(state)\n",
        "    trajectory_actions.append(song)\n",
        "    # print(counts, max_index, state)\n",
        "    for action, child in root.children.items():\n",
        "      score = ucb_score(root, child)\n",
        "      # print(child.state, score)\n",
        "  cum_rewards =  model.payoff_trajectory(trajectory_states, trajectory_actions)\n",
        "  return cum_rewards\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iddxWx7RO6_z",
        "outputId": "f29d9777-ddf5-4267-ac77-79d5fbcad4b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_9060/293283370.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self.filtered_df.dropna(inplace=True)\n"
          ]
        },
        {
          "ename": "IndexError",
          "evalue": "index 80 is out of bounds for axis 0 with size 80",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_9060/3916215948.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mplaylist_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0maction_space_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MSD.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaylist_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_space_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipykernel_9060/293283370.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_name, playlist_length, action_space_size)\u001b[0m\n\u001b[1;32m     67\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m               \u001b[0mbin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_bins\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrs_input_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m               \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 80 is out of bounds for axis 0 with size 80"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random as rand\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "import scipy.spatial.distance as distlib\n",
        "playlist_length = 10\n",
        "action_space_size = 1000\n",
        "model = MDP('MSD.csv', playlist_length, action_space_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "args = {\n",
        "    'batch_size': 16,\n",
        "    'numIters': 50,                                # Total number of training iterations\n",
        "    'num_simulations': 50,                         # Total number of MCTS simulations to run when deciding on a move to play\n",
        "    'numEps': 10,                                  # Number of full games (episodes) to run during each iteration\n",
        "    'numItersForTrainExamplesHistory': 20,\n",
        "    'epochs': 2,                                    # Number of epochs of training per iteration\n",
        "    'checkpoint_path': 'latest.pth'                 # location to save latest set of weights\n",
        "}\n",
        "\n",
        "dl_model = Connect2Model(model.n_features*model.playlist_length*10, model.n_songs, device)\n",
        "# print(model.n_songs)\n",
        "trainer = Trainer(dl_model, model, args)\n",
        "\n",
        "\n",
        "\n",
        "# Random\n",
        "random_scores = [get_random_trajectory(model) for i in range(100)]\n",
        "avg_random_score = sum(random_scores)/len(random_scores)\n",
        "\n",
        "greedy_score = get_greedy_trajectory(model)\n",
        "\n",
        "trainer.learn(1, avg_random_score, greedy_score)\n",
        "data = {\"MCTS\": trainer.scores_mcts,\n",
        "        \"MCTS_DL\": trainer.scores_mcts_dl,\n",
        "        \"GREEDY\": trainer.scores_greedy,\n",
        "        \"RANDOM\": trainer.scores_random}\n",
        "\n",
        "# with open(filepath+'{}_{}_{}_{}.pickle'.format(datetime.now().strftime(\"%y%m%d%H%M%S\"), args['numIters'], args['num_simulations'], args['numEps']), 'wb') as handle:\n",
        "#     pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(filepath+'{}_{}_{}_{}.pickle'.format(datetime.now().strftime(\"%y%m%d%H%M%S\"), args['numIters'], args['num_simulations'], args['numEps']), 'wb') as handle:\n",
        "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 912
        },
        "id": "YIqBEKY4nRZJ",
        "outputId": "840c4858-05b8-4383-be41-28a5b7dec124"
      },
      "outputs": [],
      "source": [
        "print(model.init_prefs)\n",
        "\n",
        "print(model.filtered_df)\n",
        "\n",
        "print(model.MC_value(root))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8QPL5AlAUke",
        "outputId": "6baa4a71-e79a-44db-fbec-492262a09a90"
      },
      "outputs": [],
      "source": [
        "print(model.phi_s)\n",
        "print(model.theta_s(0))\n",
        "\n",
        "print(np.dot(model.phi_s, model.theta_s(5)))\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "FoRL.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
